<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Gradient Boosting with Regression Trees</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for blackboost {mboost}"><tr><td>blackboost {mboost}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2> Gradient Boosting with Regression Trees </h2>

<h3>Description</h3>

<p>Gradient boosting for optimizing arbitrary loss functions where regression
trees are utilized as base-learners.
</p>


<h3>Usage</h3>

<pre>
blackboost(formula, data = list(),
           weights = NULL, na.action = na.pass,
           tree_controls = party::ctree_control(
               teststat = "max",
               testtype = "Teststatistic",
               mincriterion = 0,
               maxdepth = 2, savesplitstats = FALSE),
           ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>formula</code></td>
<td>
<p> a symbolic description of the model to be fit. </p>
</td></tr>
<tr valign="top"><td><code>data</code></td>
<td>
<p> a data frame containing the variables in the model. </p>
</td></tr>
<tr valign="top"><td><code>weights</code></td>
<td>
<p> an optional vector of weights to be used in the fitting
process. </p>
</td></tr>
<tr valign="top"><td><code>na.action</code></td>
<td>
<p>a function which indicates what should happen when the data
contain <code>NA</code>s. </p>
</td></tr>
<tr valign="top"><td><code>tree_controls</code></td>
<td>
<p> an object of class <code>"TreeControl"</code>, which
can be obtained using <code><a href="../../party/html/ctree_control.html">ctree_control</a></code>. Defines
hyper-parameters for the trees which are used as base-learners. It
is wise to make sure to understand the consequences of altering any
of its arguments.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>  additional arguments passed to <code><a href="mboost.html">mboost_fit</a></code>,
including <code>weights</code>, <code>offset</code>, <code>family</code> and
<code>control</code>. For default values see <code><a href="mboost.html">mboost_fit</a></code>. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the &lsquo;classical&rsquo;
gradient boosting utilizing regression trees as base-learners.
Essentially, the same algorithm is implemented in package
<code><a href="../../gbm/html/gbm.html">gbm</a></code>. The
main difference is that arbitrary loss functions to be optimized
can be specified via the <code>family</code> argument to <code>blackboost</code> whereas
<code><a href="../../gbm/html/gbm.html">gbm</a></code> uses hard-coded loss functions.
Moreover, the base-learners (conditional
inference trees, see <code><a href="../../party/html/ctree.html">ctree</a></code>) are a little bit more flexible.
</p>
<p>The regression fit is a black box prediction machine and thus
hardly interpretable.
</p>
<p>Partial dependency plots are not yet available; see example section for
plotting of additive tree models.
</p>


<h3>Value</h3>

<p>An object of class <code>mboost</code> with <code><a href="../../base/html/print.html">print</a></code>
and <code><a href="../../stats/html/predict.html">predict</a></code> methods being available.
</p>


<h3>References</h3>

<p>Peter Buehlmann and Torsten Hothorn (2007),
Boosting algorithms: regularization, prediction and model fitting.
<em>Statistical Science</em>, <b>22</b>(4), 477&ndash;505.
</p>
<p>Torsten Hothorn, Kurt Hornik and Achim Zeileis (2006). Unbiased recursive
partitioning: A conditional inference framework. <em>Journal of
Computational and Graphical Statistics</em>, <b>15</b>(3), 651&ndash;674.
</p>
<p>Yoav Freund and Robert E. Schapire (1996),
Experiments with a new boosting algorithm.
In <em>Machine Learning: Proc. Thirteenth International Conference</em>,
148&ndash;156.
</p>
<p>Jerome H. Friedman (2001),
Greedy function approximation: A gradient boosting machine.
<em>The Annals of Statistics</em>, <b>29</b>, 1189&ndash;1232.
</p>
<p>Greg Ridgeway (1999), The state of boosting.
<em>Computing Science and Statistics</em>, <b>31</b>,
172&ndash;181.
</p>


<h3>See Also</h3>

<p><code><a href="mboost.html">mboost</a></code> for the generic boosting function and
<code><a href="glmboost.html">glmboost</a></code> for boosted linear models and
<code><a href="gamboost.html">gamboost</a></code> for boosted additive models. See
<code><a href="cvrisk.html">cvrisk</a></code> for cross-validated stopping iteration.
Furthermore see <code><a href="control.html">boost_control</a></code>,  <code><a href="Family.html">Family</a></code> and
<code><a href="methods.html">methods</a></code></p>


<h3>Examples</h3>

<pre>

### a simple two-dimensional example: cars data
cars.gb &lt;- blackboost(dist ~ speed, data = cars,
                      control = boost_control(mstop = 50))
cars.gb

### plot fit
plot(dist ~ speed, data = cars)
lines(cars$speed, predict(cars.gb), col = "red")

### set up and plot additive tree model
if (require("party")) {
    ctrl &lt;- ctree_control(maxdepth = 3)
    viris &lt;- subset(iris, Species != "setosa")
    viris$Species &lt;- viris$Species[, drop = TRUE]
    imod &lt;- mboost(Species ~ btree(Sepal.Length, tree_controls = ctrl) +
                             btree(Sepal.Width, tree_controls = ctrl) +
                             btree(Petal.Length, tree_controls = ctrl) +
                             btree(Petal.Width, tree_controls = ctrl),
                   data = viris, family = Binomial())[500]
    layout(matrix(1:4, ncol = 2))
    plot(imod)
}

</pre>

<hr /><div style="text-align: center;">[Package <em>mboost</em> version 2.6-0 <a href="00Index.html">Index</a>]</div>
</body></html>
