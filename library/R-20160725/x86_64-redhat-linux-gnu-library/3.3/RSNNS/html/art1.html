<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Create and train an art1 network</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for art1 {RSNNS}"><tr><td>art1 {RSNNS}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Create and train an art1 network</h2>

<h3>Description</h3>

<p>Adaptive resonance theory (ART) networks perform clustering by finding prototypes.
They are mainly designed to solve the stability/plasticity dilemma (which is one of the
central problems in neural networks) in the following way: new input patterns
may generate new prototypes (plasticity), but patterns already present in the net
(represented by their prototypes) are only altered by similar new patterns,
not by others (stability).
ART1 is for binary inputs only,
if you have real-valued input, use <code><a href="art2.html">art2</a></code> instead.
</p>


<h3>Usage</h3>

<pre>
art1(x, ...)

## Default S3 method:
art1(x, dimX, dimY, f2Units = nrow(x), maxit = 100,
  initFunc = "ART1_Weights", initFuncParams = c(1, 1), learnFunc = "ART1",
  learnFuncParams = c(0.9, 0, 0), updateFunc = "ART1_Stable",
  updateFuncParams = c(0), shufflePatterns = TRUE, ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr valign="top"><td><code>dimX</code></td>
<td>
<p>x dimension of inputs and outputs</p>
</td></tr>
<tr valign="top"><td><code>dimY</code></td>
<td>
<p>y dimension of inputs and outputs</p>
</td></tr>
<tr valign="top"><td><code>f2Units</code></td>
<td>
<p>controls the number of clusters assumed to be present</p>
</td></tr>
<tr valign="top"><td><code>maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr valign="top"><td><code>initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr valign="top"><td><code>initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr valign="top"><td><code>learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr valign="top"><td><code>learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr valign="top"><td><code>updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr valign="top"><td><code>updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr valign="top"><td><code>shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Learning in an ART network works as follows:
A new input is intended to be classified according
to the prototypes already present in the net. The similarity between the input and
all prototypes is calculated. The most similar prototype is the <em>winner</em>.
If the similarity between the input and the winner is high enough (defined by a
<em>vigilance parameter</em>), the winner is adapted to make it more similar to the input.
If similarity is not high enough, a new prototype is created. So, at most the winner
is adapted, all other prototypes remain unchanged.
</p>
<p>The architecture of an ART network is the following:
ART is based on the more general concept of <em>competitive learning</em>. The networks have
two fully connected layers (in both directions), the input/comparison layer and the recognition layer.
They propagate activation back and forth (resonance). The units in the recognition layer have lateral
inhibition, so that they show a winner-takes-all behaviour, i.e., the unit that has the highest activation
inhibits activation of other units, so that after a few cycles its activation will converge to one, whereas
the other units activations converge to zero. ART stabilizes this general learning mechanism by the presence
of some special units. For details refer to the referenced literature.
</p>
<p>The default initialization function, <code>ART1_Weights</code>, is the only one suitable for ART1 networks. It has
two parameters, which are explained in the SNNS User Manual pp.189. A default of 1.0 for both is usually fine.
The only learning function suitable for ART1 is <code>ART1</code>. Update functions are <code>ART1_Stable</code> and
<code>ART1_Synchronous</code>. The difference between the two is that the first one updates until the network is in a
stable state, and the latter one only performs one update step. Both the learning function and the update functions
have one parameter, the vigilance parameter.
</p>
<p>In its current implementation, the network has two-dimensional input. The matrix <code>x</code> contains all
(one dimensional) input patterns. Internally, every one of these patterns
is converted to a two-dimensional pattern using parameters <code>dimX</code> and <code>dimY</code>.
The parameter <code>f2Units</code> controls the number of units in the recognition layer, and therewith the maximal amount of clusters
that are assumed to be present in the input patterns.
</p>
<p>A detailed description of the theory and the parameters is available from the SNNS documentation and the other referenced literature.
</p>


<h3>Value</h3>

<p>an <code><a href="rsnnsObjectFactory.html">rsnns</a></code> object. The <code>fitted.values</code> member of the object contains a
list of two-dimensional activation patterns.
</p>


<h3>References</h3>

<p>Carpenter, G. A. &amp; Grossberg, S. (1987), 'A massively parallel architecture for a self-organizing neural pattern recognition machine', Comput. Vision Graph. Image Process. 37, 54&ndash;115.
</p>
<p>Grossberg, S. (1988), Adaptive pattern classification and universal recoding. I.: parallel development and coding of neural feature detectors, MIT Press, Cambridge, MA, USA, chapter I, pp. 243&ndash;258.
</p>
<p>Herrmann, K.-U. (1992), 'ART &ndash; Adaptive Resonance Theory &ndash; Architekturen, Implementierung und Anwendung', Master's thesis, IPVR, University of Stuttgart. (in German)
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of TÃ¼bingen.
<a href="http://www.ra.cs.uni-tuebingen.de/SNNS/">http://www.ra.cs.uni-tuebingen.de/SNNS/</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>See Also</h3>

<p><code><a href="art2.html">art2</a></code>, <code><a href="artmap.html">artmap</a></code>
</p>


<h3>Examples</h3>

<pre>
## Not run: demo(art1_letters)
## Not run: demo(art1_lettersSnnsR)


data(snnsData)
patterns &lt;- snnsData$art1_letters.pat

inputMaps &lt;- matrixToActMapList(patterns, nrow=7)
par(mfrow=c(3,3))
for (i in 1:9) plotActMap(inputMaps[[i]])

model &lt;- art1(patterns, dimX=7, dimY=5)
encodeClassLabels(model$fitted.values)
</pre>

<hr /><div style="text-align: center;">[Package <em>RSNNS</em> version 0.4-7 <a href="00Index.html">Index</a>]</div>
</body></html>
