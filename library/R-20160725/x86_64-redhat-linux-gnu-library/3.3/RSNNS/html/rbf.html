<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: Create and train a radial basis function (RBF) network</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for rbf {RSNNS}"><tr><td>rbf {RSNNS}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Create and train a radial basis function (RBF) network</h2>

<h3>Description</h3>

<p>The use of an RBF network is similar to that of an <code><a href="mlp.html">mlp</a></code>.
The idea of radial basis function networks comes from function
interpolation theory. The RBF performs a linear combination of
n basis functions that are radially symmetric around a center/prototype.
</p>


<h3>Usage</h3>

<pre>
rbf(x, ...)

## Default S3 method:
rbf(x, y, size = c(5), maxit = 100,
  initFunc = "RBF_Weights", initFuncParams = c(0, 1, 0, 0.02, 0.04),
  learnFunc = "RadialBasisLearning", learnFuncParams = c(1e-05, 0, 1e-05,
  0.1, 0.8), updateFunc = "Topological_Order", updateFuncParams = c(0),
  shufflePatterns = TRUE, linOut = TRUE, inputsTest = NULL,
  targetsTest = NULL, ...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
<p>a matrix with training inputs for the network</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>additional function parameters (currently not used)</p>
</td></tr>
<tr valign="top"><td><code>y</code></td>
<td>
<p>the corresponding targets values</p>
</td></tr>
<tr valign="top"><td><code>size</code></td>
<td>
<p>number of units in the hidden layer(s)</p>
</td></tr>
<tr valign="top"><td><code>maxit</code></td>
<td>
<p>maximum of iterations to learn</p>
</td></tr>
<tr valign="top"><td><code>initFunc</code></td>
<td>
<p>the initialization function to use</p>
</td></tr>
<tr valign="top"><td><code>initFuncParams</code></td>
<td>
<p>the parameters for the initialization function</p>
</td></tr>
<tr valign="top"><td><code>learnFunc</code></td>
<td>
<p>the learning function to use</p>
</td></tr>
<tr valign="top"><td><code>learnFuncParams</code></td>
<td>
<p>the parameters for the learning function</p>
</td></tr>
<tr valign="top"><td><code>updateFunc</code></td>
<td>
<p>the update function to use</p>
</td></tr>
<tr valign="top"><td><code>updateFuncParams</code></td>
<td>
<p>the parameters for the update function</p>
</td></tr>
<tr valign="top"><td><code>shufflePatterns</code></td>
<td>
<p>should the patterns be shuffled?</p>
</td></tr>
<tr valign="top"><td><code>linOut</code></td>
<td>
<p>sets the activation function of the output units to linear or logistic</p>
</td></tr>
<tr valign="top"><td><code>inputsTest</code></td>
<td>
<p>a matrix with inputs to test the network</p>
</td></tr>
<tr valign="top"><td><code>targetsTest</code></td>
<td>
<p>the corresponding targets for the test input</p>
</td></tr>
</table>


<h3>Details</h3>

<p>RBF networks are feed-forward networks with one hidden layer. Their activation
is not sigmoid (as in MLP), but radially symmetric (often gaussian). Thereby,
information is represented locally in the network (in contrast to MLP, where
it is globally represented). Advantages of RBF networks in comparison to MLPs
are mainly, that the networks are more interpretable, training ought to be easier
and faster, and the network only activates in areas of the feature space where it
was actually trained, and has therewith the possibility to indicate that it &quot;just
doesn't know&quot;.
</p>
<p>Initialization of an RBF network can be difficult and require prior knowledge.
Before use of this function, you might want
to read pp 172-183 of the SNNS User Manual 4.2. The initialization is performed in
the current implementation by a call to <code>RBF_Weights_Kohonen(0,0,0,0,0)</code>
and a successive call to the given <code>initFunc</code> (usually <code>RBF_Weights</code>).
If this initialization doesn't fit your needs, you should use the RSNNS low-level interface
to implement your own one. Have a look then at the demos/examples.
Also, we note that depending on whether linear or logistic output is chosen,
the initialization parameters have to be different (normally <code>c(0,1,...)</code>
for linear and <code>c(-4,4,...)</code> for logistic output).
</p>


<h3>Value</h3>

<p>an <code><a href="rsnnsObjectFactory.html">rsnns</a></code> object.
</p>


<h3>References</h3>

<p>Poggio, T. &amp; Girosi, F. (1989), 'A Theory of Networks for Approximation and Learning'(A.I. Memo No.1140, C.B.I.P. Paper No. 31), Technical report, MIT ARTIFICIAL INTELLIGENCE LABORATORY.
</p>
<p>Vogt, M. (1992), 'Implementierung und Anwendung von Generalized Radial Basis Functions in einem Simulator neuronaler Netze', Master's thesis, IPVR, University of Stuttgart. (in German)
</p>
<p>Zell, A. et al. (1998), 'SNNS Stuttgart Neural Network Simulator User Manual, Version 4.2', IPVR, University of Stuttgart and WSI, University of TÃ¼bingen.
<a href="http://www.ra.cs.uni-tuebingen.de/SNNS/">http://www.ra.cs.uni-tuebingen.de/SNNS/</a>
</p>
<p>Zell, A. (1994), Simulation Neuronaler Netze, Addison-Wesley. (in German)
</p>


<h3>Examples</h3>

<pre>
## Not run: demo(rbf_irisSnnsR)
## Not run: demo(rbf_sin)
## Not run: demo(rbf_sinSnnsR)


inputs &lt;- as.matrix(seq(0,10,0.1))
outputs &lt;- as.matrix(sin(inputs) + runif(inputs*0.2))
outputs &lt;- normalizeData(outputs, "0_1")

model &lt;- rbf(inputs, outputs, size=40, maxit=1000,
                     initFuncParams=c(0, 1, 0, 0.01, 0.01),
                     learnFuncParams=c(1e-8, 0, 1e-8, 0.1, 0.8), linOut=TRUE)

par(mfrow=c(2,1))
plotIterativeError(model)
plot(inputs, outputs)
lines(inputs, fitted(model), col="green")
</pre>

<hr /><div style="text-align: center;">[Package <em>RSNNS</em> version 0.4-7 <a href="00Index.html">Index</a>]</div>
</body></html>
