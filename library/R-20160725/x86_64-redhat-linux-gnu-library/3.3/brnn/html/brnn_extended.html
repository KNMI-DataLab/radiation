<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd"><html xmlns="http://www.w3.org/1999/xhtml"><head><title>R: brnn_extended</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body>

<table width="100%" summary="page for brnn_extended {brnn}"><tr><td>brnn_extended {brnn}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>brnn_extended</h2>

<h3>Description</h3>

<p>The brnn_extended function
fits a two layer neural network as described in MacKay (1992) and Foresee and Hagan (1997). It uses the 
Nguyen and Widrow algorithm (1990) to assign initial weights and the Gauss-Newton algorithm to 
perform the optimization. The hidden layer contains two groups of neurons 
that allow us to assign different prior distributions for two groups of input variables.
</p>


<h3>Usage</h3>

<pre>
  brnn_extended(x, ...)

  ## S3 method for class 'formula'
brnn_extended(formula, data, contrastsx=NULL,contrastsz=NULL,...)

  ## Default S3 method:
brnn_extended(x,y,z,neurons1,neurons2,normalize=TRUE,epochs=1000,
              mu=0.005,mu_dec=0.1, mu_inc=10,mu_max=1e10,min_grad=1e-10,
              change = 0.001, cores=1,verbose =FALSE,...)
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>formula</code></td>
<td>
<p>A formula of the form <code>y ~ x1 + x2 ... | z1 + z2 ...</code>, the | is used to separate the two groups of input variables.</p>
</td></tr>
<tr valign="top"><td><code>data</code></td>
<td>
<p>Data frame from which variables specified in  <code>formula</code> are preferentially to be taken.</p>
</td></tr>
<tr valign="top"><td><code>y</code></td>
<td>
<p>(numeric, <i>n</i>) the response data-vector (NAs not  allowed).</p>
</td></tr>
<tr valign="top"><td><code>x</code></td>
<td>
<p>(numeric, <i>n x p</i>) incidence matrix for variables in group 1.</p>
</td></tr>
<tr valign="top"><td><code>z</code></td>
<td>
<p>(numeric, <i>n x q</i>) incidence matrix for variables in group 2.</p>
</td></tr>
<tr valign="top"><td><code>neurons1</code></td>
<td>
<p>positive integer that indicates the number of neurons for variables in group 1.</p>
</td></tr>
<tr valign="top"><td><code>neurons2</code></td>
<td>
<p>positive integer that indicates the number of neurons for variables in group 2.</p>
</td></tr>
<tr valign="top"><td><code>normalize</code></td>
<td>
<p>logical, if TRUE will normalize inputs and output, the default value is TRUE.</p>
</td></tr>
<tr valign="top"><td><code>epochs</code></td>
<td>
<p>positive integer, maximum number of epochs to train, default 1000.</p>
</td></tr>
<tr valign="top"><td><code>mu</code></td>
<td>
<p>positive number that controls the behaviour of the Gauss-Newton optimization algorithm, default value 0.005.</p>
</td></tr>
<tr valign="top"><td><code>mu_dec</code></td>
<td>
<p>positive number, is the mu decrease ratio, default value 0.1.</p>
</td></tr>
<tr valign="top"><td><code>mu_inc</code></td>
<td>
<p>positive number, is the mu increase ratio, default value 10.</p>
</td></tr>
<tr valign="top"><td><code>mu_max</code></td>
<td>
<p>maximum mu before training is stopped, strict positive number, default value <i>1e10</i>.</p>
</td></tr>
<tr valign="top"><td><code>min_grad</code></td>
<td>
<p>minimum gradient.</p>
</td></tr>
<tr valign="top"><td><code>change</code></td>
<td>
<p>The program  will stop if the maximum (in absolute value) of the differences of the F 
function in 3 consecutive iterations is less than this quantity.</p>
</td></tr>
<tr valign="top"><td><code>cores</code></td>
<td>
<p>Number of cpu cores to use for calculations (only available in UNIX-like operating systems). The function detectCores in the R package 
parallel can be used to attempt to detect the number of CPUs in the machine that R is running, but not necessarily 
all the cores are available for the current user, because for example in multi-user 
systems it will depend on system policies. Further details can be found in the documentation for the parallel package</p>
</td></tr>
<tr valign="top"><td><code>verbose</code></td>
<td>
<p>logical, if TRUE will print iteration history.</p>
</td></tr>
<tr valign="top"><td><code>contrastsx</code></td>
<td>
<p>an optional list of contrasts to be used for some or 
all of the factors appearing as variables in the first group of input variables in the model formula.</p>
</td></tr>
<tr valign="top"><td><code>contrastsz</code></td>
<td>
<p>an optional list of contrasts to be used for some or 
all of the factors appearing as variables in the second group of input variables in the model formula.</p>
</td></tr>
<tr valign="top"><td><code>...</code></td>
<td>
<p>arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The software fits a two layer network as described in MacKay (1992) and Foresee and Hagan (1997). 
The model is given by:
</p>
<p><i>y_i= &sum;_{k=1}^{s_1} w_k^{1} g_k (b_k^{1} + &sum;_{j=1}^p x_{ij} &beta;_j^{1[k]}) +
            &sum;_{k=1}^{s_2} w_k^{2} g_k (b_k^{2} + &sum;_{j=1}^q z_{ij} &beta;_j^{2[k]})\,\,e_i, i=1,...,n</i>
</p>

<ul>
<li><p><i>e_i ~ N(0,&sigma;_e^2)</i>.
</p>
</li>
<li><p><i>g_k(.)</i> is the activation function, in this implementation <i>g_k(x)=(exp(2x)-1)/(exp(2x)+1)</i>.
</p>
</li></ul>

<p>The software will minimize 
</p>
<p style="text-align: center;"><i>F=&beta; E_D + &alpha; &theta;_1' &theta;_1 +&delta; &theta;_2' &theta;_2 </i></p>

<p>where 
</p>

<ul>
<li><p><i>y_i-\hat y_i</i>, i.e. the sum of squared errors.
</p>
</li>
<li><p><i>beta=1/(2*sigma^2_e)</i>.
</p>
</li>
<li><p><i>alpha=1/(2*sigma_theta_1^2)</i>, <i>sigma_theta_1^2</i> is a dispersion parameter for weights and biases for the associated to 
the first group of neurons.
</p>
</li>
<li><p><i>delta=1/(2*sigma_theta_2^2)</i>, <i>sigma_theta_2^2</i> is a dispersion parameter for weights and biases for the associated to
the second group of neurons.
</p>
</li></ul>



<h3>Value</h3>

<p>object of class <code>"brnn_extended"</code> or <code>"brnn_extended.formula"</code>. Mostly internal structure, but it is a list containing:
</p>
<table summary="R valueblock">
<tr valign="top"><td><code>$theta1</code></td>
<td>
<p>A list containing weights and biases. The first <i>s_1</i> components of the list contain vectors with 
the estimated parameters for the <i>k</i>-th neuron, i.e. <i>(w_k^1, b_k^1, &beta;_1^{1[k]},...,&beta;_p^{1[k]})'</i>. 
<i>s_1</i> corresponds to neurons1 in the argument list.</p>
</td></tr>
<tr valign="top"><td><code>$theta2</code></td>
<td>
<p>A list containing weights and biases. The first <i>s_2</i> components of the list contains vectors with 
the estimated parameters for the <i>k</i>-th neuron, i.e. <i>(w_k^2, b_k^2, &beta;_1^{2[k]},...,&beta;_q^{2[k]})'</i>.
<i>s_2</i> corresponds to neurons2 in the argument list.</p>
</td></tr>
<tr valign="top"><td><code>$message</code></td>
<td>
<p>String that indicates the stopping criteria for the training process.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Foresee, F. D., and M. T. Hagan. 1997. &quot;Gauss-Newton approximation to Bayesian regularization&quot;, 
<em>Proceedings of the 1997 International Joint Conference on Neural Networks</em>.
</p>
<p>MacKay, D. J. C. 1992. &quot;Bayesian interpolation&quot;, <em>Neural Computation</em>, vol. 4, no. 3, pp. 415-447.
</p>
<p>Nguyen, D. and Widrow, B. 1990. &quot;Improving the learning speed of 2-layer neural networks by choosing initial values of the adaptive weights&quot;,
<em>Proceedings of the IJCNN</em>, vol. 3, pp. 21-26.
</p>


<h3>See Also</h3>

<p><code><a href="predictions.brnn_extended.html">predict.brnn_extended</a></code>
</p>


<h3>Examples</h3>

<pre>

## Not run: 

#Example 5
#Warning, it will take a while

#Load the Jersey dataset
data(Jersey)

#Predictive power of the model using the SECOND set for 10 fold CROSS-VALIDATION
data=pheno
data$G=G
data$D=D
data$partitions=partitions

#Fit the model for the TESTING DATA for Additive + Dominant
out=brnn_extended(yield_devMilk ~ G | D,
                                  data=subset(data,partitions!=2),
                                  neurons1=2,neurons2=2,epochs=100,verbose=TRUE)

#Plot the results
#Predicted vs observed values for the training set
par(mfrow=c(2,1))
yhat_R_training=predict(out)
plot(out$y,yhat_R_training,xlab=expression(hat(y)),ylab="y")
cor(out$y,yhat_R_training)

#Predicted vs observed values for the testing set
newdata=subset(data,partitions==2,select=c(D,G))
ytesting=pheno$yield_devMilk[partitions==2]
yhat_R_testing=predict(out,newdata=newdata)
plot(ytesting,yhat_R_testing,xlab=expression(hat(y)),ylab="y")
cor(ytesting,yhat_R_testing)
  

## End(Not run)
 
</pre>

<hr /><div style="text-align: center;">[Package <em>brnn</em> version 0.6 <a href="00Index.html">Index</a>]</div>
</body></html>
